{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb898666",
   "metadata": {},
   "source": [
    "# Linear Regression: Salary Prediction\n",
    "\n",
    "This notebook demonstrates the full workflow for fitting an ordinary least squares model to the salary dataset contained in this module. We will focus on sound exploratory data analysis, statistical validation, and reusable training code that mirrors the `src/` implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de103db",
   "metadata": {},
   "source": [
    "## 1. Mathematical refresher\n",
    "\n",
    "Ordinary least squares finds coefficients $\\beta$ that minimise the residual sum of squares: $\\min_\\beta \\|y - X\\beta\\|^2$. When $X$ has full column rank the solution admits a closed form, $\\hat{\\beta} = (X^\\top X)^{-1} X^\\top y$. The residual diagnostics that follow will help us check linearity, homoscedasticity, and normality assumptions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab71a78",
   "metadata": {},
   "source": [
    "## 2. Dataset and experiment plan\n",
    "\n",
    "\\n\n",
    "We work with `salary_data.csv`, a compact example that maps years of professional experience to annual salary. Even though the dataset is tiny, it is perfect for illustrating:\n",
    "\n",
    "\n",
    "\n",
    "- deterministic train/validation splits for reproducibility;\n",
    "\n",
    "- feature scaling inside a pipeline so training and inference use the same logic;\n",
    "\n",
    "- lightweight evaluation metrics that we can compare with the numbers emitted by `src/train.py`.\n",
    "\n",
    "\n",
    "\n",
    "> **Tip:** Notebook experiments should mirror the production configuration. The helper utilities in `src/` will ensure that any improvements made here (e.g. switching to Ridge regression) can be ported back with minimal code changes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7574e5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "DATA_PATH = Path('..') / 'data' / 'salary_data.csv'\n",
    "DATA_PATH.resolve()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4d1799",
   "metadata": {},
   "source": [
    "The pipeline components we import here match the definitions in `src/pipeline.py`. Keeping both sides aligned ensures that the experiment you run locally is identical to the code path that the automated training routine executes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb36a9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46985ab",
   "metadata": {},
   "source": [
    "The raw dataset contains only two columns, so exploratory work focuses on spotting outliers and understanding the linear relationship. With larger datasets you would extend this section to include correlation matrices, missing-value audits, and feature engineering sketches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01425b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploratory plots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "sns.scatterplot(data=df, x='YearsExperience', y='Salary', ax=axes[0])\n",
    "axes[0].set_title('Salary vs. Experience')\n",
    "sns.histplot(df['Salary'], kde=True, ax=axes[1])\n",
    "axes[1].set_title('Salary distribution')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16d1b26",
   "metadata": {},
   "source": [
    "We reserve 20% of the observations for validation. Because the dataset is tiny, consider running multiple random seeds or K-fold cross-validation when you expand the module; the helper functions in `src/data.py` can be adapted accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f99cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train / validation split\n",
    "X = df[['YearsExperience']]\n",
    "y = df['Salary']\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train.shape, X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e125097f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the same pipeline used in src/pipeline.py\n",
    "numeric_features = ['YearsExperience']\n",
    "numeric_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "])\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('numeric', numeric_pipeline, numeric_features),\n",
    "])\n",
    "regression_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', LinearRegression()),\n",
    "])\n",
    "regression_pipeline.fit(X_train, y_train)\n",
    "y_pred = regression_pipeline.predict(X_val)\n",
    "r2 = r2_score(y_val, y_pred)\n",
    "rmse = mean_squared_error(y_val, y_pred, squared=False)\n",
    "mae = mean_absolute_error(y_val, y_pred)\n",
    "{'r2': r2, 'rmse': rmse, 'mae': mae}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9240e5a6",
   "metadata": {},
   "source": [
    "The metrics dictionary should align with the contents of `artifacts/metrics.json` generated by the Python training script. Use this parity check to ensure the notebook and pipeline stay in sync whenever you tweak preprocessing or model choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7d6a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual diagnostics\n",
    "residuals = y_val - y_pred\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "sns.scatterplot(x=y_pred, y=residuals, ax=axes[0])\n",
    "axes[0].axhline(0.0, color='black', linestyle='--')\n",
    "axes[0].set_title('Residuals vs. Fitted')\n",
    "sns.histplot(residuals, kde=True, ax=axes[1])\n",
    "axes[1].set_title('Residual distribution')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db3862f",
   "metadata": {},
   "source": [
    "## 3. Serving the model\n",
    "\n",
    "\n",
    "\n",
    "The notebook complements the production code in `src/`. After validating an idea here, run:\n",
    "\n",
    "\n",
    "\n",
    "```bash\n",
    "\n",
    "python \"Supervised Learning/Linear Regression/src/train.py\"\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "This command retrains the model, refreshes artifacts, and makes the FastAPI endpoint immediately pick up the latest weights via the shared registry. Start the API with:\n",
    "\n",
    "\n",
    "\n",
    "```bash\n",
    "\n",
    "python -m fastapi_app.main\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "You can then send a POST request to `http://localhost:8000/models/linear_regression` with a JSON payload like `{\"years_experience\": 6.5}` or explore the interactive docs at `/docs`.\n",
    "\n",
    "\n",
    "\n",
    "As you replicate this structure for other algorithms, mirror the three pillars shown here: rich markdown explanations, reproducible experiments, and modular code that production services can import without drifting from the research findings."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
