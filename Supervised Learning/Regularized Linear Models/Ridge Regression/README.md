# Ridge Regression — Regularised Linear Baseline

**Location:** `Machine-Learning/Supervised Learning/Regularized Linear Models/Ridge Regression`

Ridge regression augments ordinary least squares with an L2 penalty, shrinking coefficients to improve generalisation when multicollinearity or high-dimensional features are present. This module mirrors the standard repository workflow so you can train, evaluate, and serve ridge models alongside other supervised pipelines.

## Learning Objectives

- Diagnose multicollinearity and understand how the L2 penalty stabilises the solution.
- Train a scikit-learn `Ridge` pipeline with scaling baked in.
- Sweep the regularisation strength (`alpha`) and log metrics for comparison against OLS.
- Package the trained model for FastAPI inference just like existing modules.

## Project Layout

```
Ridge Regression/
├── README.md                # You are here
├── data/                    # Regression dataset with collinear features
├── notebooks/               # Exploratory analysis and hyperparameter tuning
├── src/
│   ├── config.py            # Dataclass for hyperparameters and paths
│   ├── data.py              # Data loading and train/validation split helpers
│   ├── pipeline.py          # Ridge training pipeline and persistence helpers
│   ├── train.py             # CLI for training and logging metrics
│   └── inference.py         # FastAPI-ready service wrapper
├── demo.py                  # Minimal script invoking the trained artefacts
└── artifacts/               # Model weights, metrics, and model card
```

## Quickstart

1. Install dependencies shared across the repo:
   ```bash
   pip install -r requirements.txt
   ```
2. Train the ridge baseline:
   ```bash
   python "Supervised Learning/Regularized Linear Models/Ridge Regression/src/train.py"
   ```
3. Inspect `artifacts/metrics.json` to compare against the OLS module and adjust `alpha` accordingly.
4. Launch the FastAPI gateway (`python -m fastapi_app.main`) and use the autogenerated `/models/ridge_regression` endpoint for live inference.

## Dataset

The `data/` folder includes a synthetic salary-style dataset extended with correlated features to highlight where ridge excels. Replace it with your own CSV and update `config.py` to experiment with alternative feature sets.

## Next Steps

- Integrate cross-validation using `RidgeCV` and surface the best `alpha` in metrics.
- Extend the notebooks with coefficient path plots showing how weights shrink.
- Compare downstream calibration performance against logistic regression via the Calibration module.
