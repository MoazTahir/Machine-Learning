{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7be66b8b",
   "metadata": {},
   "source": [
    "# Support Vector Regression â€” California Housing Notebook\n",
    "\n",
    "This notebook mirrors the production SVR pipeline in `src/` and provides an interactive space for dataset inspection, training diagnostics, and hyperparameter experiments. Work through the sections sequentially to keep results aligned with the scripted workflow.\n",
    "\n",
    "**Roadmap**\n",
    "\n",
    "- Load the cached dataset and confirm feature ordering.\n",
    "- Recreate the quantile-stratified train/validation split.\n",
    "- Train the SVR pipeline, persist artefacts, and validate metrics.\n",
    "- Visualise residuals, error distributions, and kernel behaviour.\n",
    "- Capture experiment notes for future improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90311fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Environment imports aligned with the production pipeline.\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.metrics import PredictionErrorDisplay\n",
    "\n",
    "from src.config import CONFIG as SVR_CONFIG, SVRConfig\n",
    "from src.data import load_dataset, build_features, train_validation_split\n",
    "from src.pipeline import CaliforniaHousingSVRPipeline, train_and_persist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2fb1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"whitegrid\")\n",
    "config: SVRConfig = SVR_CONFIG\n",
    "raw_df = load_dataset(config)\n",
    "display(raw_df.head())\n",
    "print(f\"Total rows: {len(raw_df):,}\")\n",
    "print(\"Missing values per column:\")\n",
    "display(raw_df.isna().sum().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2bbf18",
   "metadata": {},
   "source": [
    "## 1. Dataset Overview\n",
    "\n",
    "Columns are normalised to snake_case so they align with `SVRConfig.feature_columns`. The target `median_house_value` is stored in $100k units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f03f85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = build_features(raw_df, config)\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(\"Target summary:\")\n",
    "display(y.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f7adf4",
   "metadata": {},
   "source": [
    "### Correlation snapshot\n",
    "\n",
    "ETL sanity check for multicollinearity before fitting SVR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1485db",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = X.corr()\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(corr, annot=False, cmap=\"coolwarm\", square=True)\n",
    "plt.title(\"Feature correlation heatmap\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14c7721",
   "metadata": {},
   "source": [
    "## 2. Train/Validation Split\n",
    "\n",
    "Replicate the quantile-stratified 80/20 split used by `src/train.py` to keep notebook metrics in sync with the CLI workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c967b949",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_validation_split(config)\n",
    "print(f\"Train size: {X_train.shape[0]:,} | Validation size: {X_val.shape[0]:,}\")\n",
    "print(\"Target quantiles (train):\")\n",
    "display(y_train.quantile([0.1, 0.5, 0.9]))\n",
    "print(\"Target quantiles (validation):\")\n",
    "display(y_val.quantile([0.1, 0.5, 0.9]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac941a8",
   "metadata": {},
   "source": [
    "## 3. Train the Production Pipeline\n",
    "\n",
    "Instantiate `CaliforniaHousingSVRPipeline`, fit on the training fold, and persist artefacts. rerun this cell after tweaking hyperparameters or preprocessing to regenerate weights and metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e184fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = CaliforniaHousingSVRPipeline(config)\n",
    "metrics = pipeline.train()\n",
    "artifact_path = pipeline.save()\n",
    "metrics_path = pipeline.write_metrics(metrics)\n",
    "print(\"Training metrics:\")\n",
    "display(metrics)\n",
    "print(f\"Model artifact: {artifact_path}\")\n",
    "print(f\"Metrics file: {metrics_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d49e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_pred = pipeline.pipeline.predict(X_val)\n",
    "eval_metrics = {\n",
    "    'r2': float(r2_score(y_val, y_val_pred)),\n",
    "    'rmse': float(np.sqrt(mean_squared_error(y_val, y_val_pred))),\n",
    "    'mae': float(mean_absolute_error(y_val, y_val_pred)),\n",
    "}\n",
    "display(eval_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8721e58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "axes[0].hist(y_val - y_val_pred, bins=40, color=\"steelblue\", alpha=0.8)\n",
    "axes[0].set_title(\"Residual histogram\")\n",
    "axes[0].set_xlabel(\"Error (actual - predicted)\")\n",
    "axes[0].set_ylabel(\"Frequency\")\n",
    "PredictionErrorDisplay.from_predictions(y_val, y_val_pred, kind=\"actual_vs_predicted\", ax=axes[1])\n",
    "axes[1].plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], linestyle='--', color='grey', alpha=0.6)\n",
    "axes[1].set_title(\"Actual vs. Predicted\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41743f9a",
   "metadata": {},
   "source": [
    "## 4. Support Vector Diagnostics\n",
    "\n",
    "Although SVR does not expose class counts, examining the number of support vectors helps gauge model complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f85c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "svr = pipeline.pipeline.named_steps['regressor']\n",
    "print(f\"Support vectors: {svr.support_.shape[0]}\")\n",
    "print('Dual coefficients summary:')\n",
    "display(pd.Series(svr.dual_coef_[0]).describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fe3a85",
   "metadata": {},
   "source": [
    "## 5. Experiment Log\n",
    "\n",
    "- **Parameter sweeps**: log results for different combinations of `C`, `epsilon`, and kernel choices.\n",
    "- **Feature engineering**: evaluate log-scaled population or derived ratios (rooms per bedroom) and note metric shifts.\n",
    "- **Monitoring**: track rolling RMSE on fresh validation slices to detect drift.\n",
    "- **Prediction intervals**: experiment with residual bootstrapping or conformal methods to provide uncertainty estimates.\n",
    "- **Batch scoring**: document CLI or Airflow jobs that reuse `CaliforniaHousingService` for offline inference."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
